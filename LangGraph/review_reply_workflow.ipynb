{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-03T02:49:54.993983200Z",
     "start_time": "2026-01-03T02:49:53.261501500Z"
    }
   },
   "source": [
    "from langgraph.graph import StateGraph,START,END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\LangChain\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T02:50:46.110607400Z",
     "start_time": "2026-01-03T02:50:45.536291600Z"
    }
   },
   "cell_type": "code",
   "source": "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",temperature=0)",
   "id": "d920d832095e8561",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T02:53:10.346714300Z",
     "start_time": "2026-01-03T02:53:10.315822700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sentiment(BaseModel):\n",
    "    sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"Sentiment of the review\")"
   ],
   "id": "874d7cb2c0b9a9cf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Diagnosis(BaseModel):\n",
    "    issue_type: Literal[\"UX\",\"Bug\",\"Support\",\"Other\"] = Field(description=\"Type of issue\")\n",
    "    tone: Literal[\"angry\",\"frustrated\",\"disappointed\",\"calm\"] = Field(description=\"Tone of the review\")\n",
    "    urgency: Literal[\"low\",\"medium\",\"high\"] = Field(description=\"How urgent or critical is the issue appears to be\")"
   ],
   "id": "b9c24337e14019f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T02:53:11.492934200Z",
     "start_time": "2026-01-03T02:53:11.467264500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "structured_llm = llm.with_structured_output(Sentiment)\n",
    "structured_llm2 = llm.with_structured_output(Diagnosis)"
   ],
   "id": "2d0d4616015167c0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T04:05:01.403848900Z",
     "start_time": "2026-01-03T04:05:01.363886300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReviewState(TypedDict):\n",
    "    review: str\n",
    "    sentiment: Literal[\"positive\", \"negative\"]\n",
    "    diagnosis: dict\n",
    "    response: str"
   ],
   "id": "96602fcb2f8e120e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T04:05:03.020836Z",
     "start_time": "2026-01-03T04:05:03.000369300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_sentiment(state: ReviewState):\n",
    "    prompt = f'for the following review: \"{state[\"review\"]}\" what is the sentiment?'\n",
    "    sentimenet = structured_llm.invoke(prompt)\n",
    "    return {\"sentiment\": sentimenet.sentiment}\n",
    "\n",
    "def check_sentiment(state: ReviewState) -> Literal[\"positive_feedback\", \"run_diagnosis\"]:\n",
    "    if state['sentiment'] == 'positive':\n",
    "        return 'positive_feedback'\n",
    "    else:\n",
    "        return 'run_diagnosis'\n",
    "\n",
    "def positive_response(state: ReviewState):\n",
    "    prompt = f\"\"\"Write a warm thank-you message in response to the review: \\n\\n\\ \"{state['review']}\\\"\\n\n",
    "    Also, kindly ask the user to leave a feedback on our website.\n",
    "\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"response\": response}\n",
    "\n",
    "def run_diagnosis(state: ReviewState):\n",
    "    prompt = f\"\"\"Diagnose this negative review:\\n\\n{state['review']}\\n\n",
    "    Return issue_type, tone, urgency.\n",
    "\n",
    "\"\"\"\n",
    "    response = structured_llm2.invoke(prompt)\n",
    "    return {\"diagnosis\": response.model_dump()}\n",
    "\n",
    "def negative_response(state: ReviewState):\n",
    "    diagnosis = state[\"diagnosis\"]\n",
    "    prompt = f\"\"\"You are a support assistant.\n",
    "    The user had a '{diagnosis['issue_type']}' issue, sounded '{diagnosis['tone']}', and marked urgentcy as '{diagnosis['urgency']}'. Write an empathetic, helpful response to the user.\n",
    "\n",
    "\"\"\"\n",
    "    response = llm.invoke(prompt).content\n",
    "    return {'response':response}\n",
    "\n"
   ],
   "id": "48005c40a23071db",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "19a03bf3bc09e7d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "graph = StateGraph(ReviewState)\n",
    "\n",
    "graph.add_node('find_sentiment',find_sentiment)\n",
    "graph.add_node('run_diagnosis',run_diagnosis)\n",
    "graph.add_node('negative_response',negative_response)\n",
    "\n",
    "graph.add_edge(START, 'find_sentiment')\n",
    "graph.add_conditional_edges('find_sentiment',check_sentiment)\n",
    "graph.add_edge('run_diagnosis', 'negative_response')\n",
    "graph.add_edge('negative_response', END)\n",
    "graph.add_edge('positive_response',END)\n",
    "graph.add_edge('find_sentiment',END)\n",
    "\n",
    "workflow = graph.compile()\n",
    "workflow\n"
   ],
   "id": "2c08ac5c24fe394",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f4d655b383936902"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T02:53:49.473651100Z",
     "start_time": "2026-01-03T02:53:12.736519300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"You guys suck. No great products\"\n",
    "structured_llm.invoke(prompt)"
   ],
   "id": "d9cb58738328c8e1",
   "outputs": [
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Error calling model 'gemini-2.0-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 10.302291255s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '10s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mClientError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:3040\u001B[39m, in \u001B[36mChatGoogleGenerativeAI._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001B[39m\n\u001B[32m   3039\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3040\u001B[39m     response: GenerateContentResponse = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmodels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate_content\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3041\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3042\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3043\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ClientError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\google\\genai\\models.py:5203\u001B[39m, in \u001B[36mModels.generate_content\u001B[39m\u001B[34m(self, model, contents, config)\u001B[39m\n\u001B[32m   5202\u001B[39m i += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m5203\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_content\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5204\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontents\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcontents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparsed_config\u001B[49m\n\u001B[32m   5205\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5207\u001B[39m function_map = _extra_utils.get_function_map(parsed_config)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\google\\genai\\models.py:3985\u001B[39m, in \u001B[36mModels._generate_content\u001B[39m\u001B[34m(self, model, contents, config)\u001B[39m\n\u001B[32m   3983\u001B[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001B[32m-> \u001B[39m\u001B[32m3985\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_api_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3986\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mpost\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhttp_options\u001B[49m\n\u001B[32m   3987\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3989\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\n\u001B[32m   3990\u001B[39m     config, \u001B[33m'\u001B[39m\u001B[33mshould_return_http_response\u001B[39m\u001B[33m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   3991\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1388\u001B[39m, in \u001B[36mBaseApiClient.request\u001B[39m\u001B[34m(self, http_method, path, request_dict, http_options)\u001B[39m\n\u001B[32m   1385\u001B[39m http_request = \u001B[38;5;28mself\u001B[39m._build_request(\n\u001B[32m   1386\u001B[39m     http_method, path, request_dict, http_options\n\u001B[32m   1387\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1388\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp_request\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhttp_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   1389\u001B[39m response_body = (\n\u001B[32m   1390\u001B[39m     response.response_stream[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m response.response_stream \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m   1391\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1222\u001B[39m, in \u001B[36mBaseApiClient._request\u001B[39m\u001B[34m(self, http_request, http_options, stream)\u001B[39m\n\u001B[32m   1221\u001B[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001B[32m-> \u001B[39m\u001B[32m1222\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mretry\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request_once\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhttp_request\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[no-any-return]\u001B[39;00m\n\u001B[32m   1224\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._retry(\u001B[38;5;28mself\u001B[39m._request_once, http_request, stream)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001B[39m, in \u001B[36mRetrying.__call__\u001B[39m\u001B[34m(self, fn, *args, **kwargs)\u001B[39m\n\u001B[32m    476\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m477\u001B[39m     do = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    478\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001B[39m, in \u001B[36mBaseRetrying.iter\u001B[39m\u001B[34m(self, retry_state)\u001B[39m\n\u001B[32m    377\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m action \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.iter_state.actions:\n\u001B[32m--> \u001B[39m\u001B[32m378\u001B[39m     result = \u001B[43maction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    379\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001B[39m, in \u001B[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001B[39m\u001B[34m(rs)\u001B[39m\n\u001B[32m    419\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.reraise:\n\u001B[32m--> \u001B[39m\u001B[32m420\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[43mretry_exc\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    421\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m retry_exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mfut\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexception\u001B[39;00m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001B[39m, in \u001B[36mRetryError.reraise\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    186\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.last_attempt.failed:\n\u001B[32m--> \u001B[39m\u001B[32m187\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlast_attempt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python314\\Lib\\concurrent\\futures\\_base.py:443\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    442\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state == FINISHED:\n\u001B[32m--> \u001B[39m\u001B[32m443\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    445\u001B[39m \u001B[38;5;28mself\u001B[39m._condition.wait(timeout)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python314\\Lib\\concurrent\\futures\\_base.py:395\u001B[39m, in \u001B[36mFuture.__get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    394\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m395\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception\n\u001B[32m    396\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    397\u001B[39m     \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001B[39m, in \u001B[36mRetrying.__call__\u001B[39m\u001B[34m(self, fn, *args, **kwargs)\u001B[39m\n\u001B[32m    479\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m480\u001B[39m     result = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    481\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1201\u001B[39m, in \u001B[36mBaseApiClient._request_once\u001B[39m\u001B[34m(self, http_request, stream)\u001B[39m\n\u001B[32m   1194\u001B[39m response = \u001B[38;5;28mself\u001B[39m._httpx_client.request(\n\u001B[32m   1195\u001B[39m     method=http_request.method,\n\u001B[32m   1196\u001B[39m     url=http_request.url,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1199\u001B[39m     timeout=http_request.timeout,\n\u001B[32m   1200\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1201\u001B[39m \u001B[43merrors\u001B[49m\u001B[43m.\u001B[49m\u001B[43mAPIError\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraise_for_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1202\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m HttpResponse(\n\u001B[32m   1203\u001B[39m     response.headers, response \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m [response.text]\n\u001B[32m   1204\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\google\\genai\\errors.py:121\u001B[39m, in \u001B[36mAPIError.raise_for_response\u001B[39m\u001B[34m(cls, response)\u001B[39m\n\u001B[32m    119\u001B[39m   response_json = response.body_segments[\u001B[32m0\u001B[39m].get(\u001B[33m'\u001B[39m\u001B[33merror\u001B[39m\u001B[33m'\u001B[39m, {})\n\u001B[32m--> \u001B[39m\u001B[32m121\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mraise_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_json\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\google\\genai\\errors.py:146\u001B[39m, in \u001B[36mAPIError.raise_error\u001B[39m\u001B[34m(cls, status_code, response_json, response)\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[32m400\u001B[39m <= status_code < \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m146\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m ClientError(status_code, response_json, response)\n\u001B[32m    147\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[32m500\u001B[39m <= status_code < \u001B[32m600\u001B[39m:\n",
      "\u001B[31mClientError\u001B[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 10.302291255s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '10s'}]}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mChatGoogleGenerativeAIError\u001B[39m               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m prompt = \u001B[33m\"\u001B[39m\u001B[33mYou guys suck. No great products\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mstructured_llm\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3149\u001B[39m, in \u001B[36mRunnableSequence.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   3147\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m set_config_context(config) \u001B[38;5;28;01mas\u001B[39;00m context:\n\u001B[32m   3148\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i == \u001B[32m0\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m3149\u001B[39m         input_ = \u001B[43mcontext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3150\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3151\u001B[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5557\u001B[39m, in \u001B[36mRunnableBindingBase.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   5550\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   5551\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m   5552\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   5555\u001B[39m     **kwargs: Any | \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   5556\u001B[39m ) -> Output:\n\u001B[32m-> \u001B[39m\u001B[32m5557\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbound\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5558\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   5559\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_merge_configs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5560\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43m{\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5561\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2529\u001B[39m, in \u001B[36mChatGoogleGenerativeAI.invoke\u001B[39m\u001B[34m(self, input, config, code_execution, stop, **kwargs)\u001B[39m\n\u001B[32m   2526\u001B[39m         msg = \u001B[33m\"\u001B[39m\u001B[33mTools are already defined.code_execution tool can\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt be defined\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2527\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[32m-> \u001B[39m\u001B[32m2529\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001B[39m, in \u001B[36mBaseChatModel.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    384\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    385\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    386\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    391\u001B[39m     **kwargs: Any,\n\u001B[32m    392\u001B[39m ) -> AIMessage:\n\u001B[32m    393\u001B[39m     config = ensure_config(config)\n\u001B[32m    394\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    395\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAIMessage\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    396\u001B[39m         cast(\n\u001B[32m    397\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mChatGeneration\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m--> \u001B[39m\u001B[32m398\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    399\u001B[39m \u001B[43m                \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    400\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    401\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    402\u001B[39m \u001B[43m                \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    403\u001B[39m \u001B[43m                \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    404\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_name\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    405\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    406\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    407\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m.generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m],\n\u001B[32m    408\u001B[39m         ).message,\n\u001B[32m    409\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001B[39m, in \u001B[36mBaseChatModel.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m   1108\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   1109\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m   1110\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1114\u001B[39m     **kwargs: Any,\n\u001B[32m   1115\u001B[39m ) -> LLMResult:\n\u001B[32m   1116\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m-> \u001B[39m\u001B[32m1117\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001B[39m, in \u001B[36mBaseChatModel.generate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    924\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[32m    925\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    926\u001B[39m         results.append(\n\u001B[32m--> \u001B[39m\u001B[32m927\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    928\u001B[39m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    929\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    933\u001B[39m         )\n\u001B[32m    934\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    935\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001B[39m, in \u001B[36mBaseChatModel._generate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1219\u001B[39m     result = generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[32m   1220\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._generate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1221\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1222\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m   1223\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1224\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1225\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:3044\u001B[39m, in \u001B[36mChatGoogleGenerativeAI._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001B[39m\n\u001B[32m   3040\u001B[39m     response: GenerateContentResponse = \u001B[38;5;28mself\u001B[39m.client.models.generate_content(\n\u001B[32m   3041\u001B[39m         **request,\n\u001B[32m   3042\u001B[39m     )\n\u001B[32m   3043\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ClientError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m-> \u001B[39m\u001B[32m3044\u001B[39m     \u001B[43m_handle_client_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3046\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _response_to_result(response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\LangChain\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:145\u001B[39m, in \u001B[36m_handle_client_error\u001B[39m\u001B[34m(e, request)\u001B[39m\n\u001B[32m    143\u001B[39m model_name = request.get(\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33munknown\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    144\u001B[39m msg = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError calling model \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00me.status\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m145\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m ChatGoogleGenerativeAIError(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n",
      "\u001B[31mChatGoogleGenerativeAIError\u001B[39m: Error calling model 'gemini-2.0-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 10.302291255s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '10s'}]}}"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
